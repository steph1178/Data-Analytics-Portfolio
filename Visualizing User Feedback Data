# import all modules needed
import redshift_connector
import pandas as pd
import nltk
from nltk import pos_tag, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# open engine connection: conn
# grab credentials from .env file
conn = redshift_connector.connect(
     host='host',
     database='database',
     user='name',
     password='password'
  )

cursor = conn.cursor()
# perform query
cursor.execute(
    """
    select 
        user_survey_id,
        question_141
    from survey_results
    where question_141 is not null;
    """
)

# save the results of the query to a pandas df
df: pd.DataFrame = cursor.fetch_dataframe()

# close the connection to redshift
conn.close()

# standard cleaning of data:lowercase all and strip whitespace
org_feedback = df['question_141'].apply(lambda x: x.strip().lower())

# custom clean up of html taggings and convert feedback to a string
org_feedback= org_feedback.str.replace("<p>", "").str.replace("</p>", "").str.\
    replace("<em>", "").str.replace("</em>", "")
org_feedback = org_feedback.astype(str)

# replace symbols: this is also stripping hyphenated words like self-reflection
# r' matches char literally(case sensitive)
# \w matches any alpha numeric, \s any whitespace character
# + matches the previous token between one and unlimited times, as many times as possible, giving back as needed
org_feedback = org_feedback.str.replace(r'[^\w\s]+', '')
   

# tokenize feedback
tokens = [word_tokenize(str(sentence)) for sentence in org_feedback]

# flatten sublists
flattened_tokens = [val for sublist in tokens for val in sublist]

# # remove stop words
stopwords = stopwords.words('english')
# optional code snippet left in as reference for if more stop words are needed
# newStopWords = ['session', 'sessions', 'meeting']
# stopwords.extend(newStopWords)
stopwords_removed = [w for w in flattened_tokens if w not in stopwords]


# remove unwanted parts of speech
tagged_words = pos_tag(stopwords_removed)
pos_removed = [w for w in tagged_words if w[1] not in ('CC','IN','DT','TO')]
org_feedback = [item[0] for item in pos_removed]


# lemmatize-need to use pos = 'v' tag to lemmetize verbs in addition to nouns (default is nouns only)
lemmatizer = WordNetLemmatizer()
feedback = [lemmatizer.lemmatize(w, pos='v') for w in org_feedback]

# word count frequency top ten
counts = Counter(feedback).most_common(10)

# create a wordcloud
wordcloud = WordCloud(
    background_color='white',
    max_words=100,
    max_font_size=40).generate(str(' '.join(feedback)))
fig = plt.figure(1, figsize=(20, 20))
plt.axis("off")
plt.imshow(wordcloud)
plt.show()

# convert word count list to DF and Plot horizontal bar using matplotlib bar().
userReviews = pd.DataFrame(counts,
                           columns=['words', 'count'])
fig, ax = plt.subplots(figsize=(8, 8))
userReviews.sort_values(by='count').plot.barh(x='words',
                                              y='count',
                                              ax=ax,
                                              color='purple')
ax.set_title("Most common words in NPS feedback")
plt.show()
